10/19(수) 복습
문제정의
데이터 준비
데이터 전처리(EDA 등등)
train-test split
encoding
모델 선언: model=KNeighborsClassifier(n_neighbors = i)와 같은 형태
          단점: 잘 안쓰임(학습은 빠르지만 예측이 느림)
모델 학습: model.fit(x_train, y_train)와 같은 형태
          딥러닝에서는 model.eval()
모델 평가: model.predict(x_test)와 같은 형태

결과: model.score(X_test, y_test)
최적화: predict를 통해서 나온 점수와 하이퍼파라미터를 변경한 후 나온 점수를 계속해서 비교해서 찾아냄?

tip.모델의 특징을 잘 알아두자!

-----------복습 시작----------------
분류: 정확도(높을수록 좋음) -> precision, recall, flsocore, roc, auc
회귀: mse(낮을수록 좋음), R^2(높을수록 좋음)

KNN: 분류, 기하 / HP-k / 직관적이고 학습은 빠르지만 예측이 느림

최적화 단계에서 과대적합/과소적합일 때 여러 방법을 통해서 predict를 통해 나온 점수를 개선한다.

overfitting(과대적합): 너무 한 분야만 학습을 많이해서 다른 분야를 물어봤을때 잘 모르는 경우

underfitting(과소적합): 아직을 학습을 많이 못해서 나타나는 경우(ex.신입사원)

과대적합인지 과소적합인지 잘 모르는 경우 이용할 수 있는 방법
-> train_acc와 test_acc를 비교
    - 과대적합train_acc와 test_acc의 격차가 클때
        - 과대적합이 아닌 조건: Train Accuracy와 Test Accuracy의 차이가 적다
    - 과소적합 train_acc와 test_acc 양쪽 모두 예측 성능이 저하되는 현상
        - 과소적합이 아닌 조건: Test Accuracy가 높다

Bias-Variance Trade-Off
Bias(높): 데이터가 모여있지만(수렴은 하지만) 목표치에서는 벗어난 상태(과대적합)
          예측값 - 측정값(이 값이 클 수록 bias가 크다)
Variance(높): 한 점에 수렴하지 못하고 퍼져있는 상태(과소적합)
              측정값의 평균 - 측정값
최적의 상태: 목표치에 수렴한 상태
최악: bias와 variance 둘 다 높을 때(과소적합)

분류모델: KNN(k번째 가장 가까운 이웃(Kth Nearest Neighbor))
         -기하적 모델: 단위의 영향을 크게 받는다.
         - K = 주변의 비교할 데이터의 개수(권장사항: 레이블의 배수가 되지 않도록 할 것) <- 하이퍼파라미터
         - K를 너무 적게 하면 근처의 1개만 비교하고(과대적합) 너무 많이 하면 굳이 KNN을 할 필요가 없음(과소적합)
         - 유클리디안 거리(*)/ 맨해튼 거리 
         - Scaling 
            - standard scaling: 평균이 0, 편차가 1인 데이터셋으로 만드는 것 (x-m / σ) 
            - min-max scaling: 데이터를 0과 1사이 값으로 만드는 것 (x-min / max-min)

회귀모델: 선형회귀 - 회귀, 기하 / HP: 존재하지 않음
            1.선형회귀는 지도학습이다.(회귀)
            2.선형회귀를 평가하는 지표는 mse(mse는 작을수록 좋음)
            3.최적의 선형회귀를 그리기 위한 방법 2가지
                - mse를 미분하는 방법: 계산
                - 경사하강법: 노가다(데이터량이 많을수록 유리, 특성의 개수가 많아도 유리)
            4.경사하강법의 3가지
                - 일반적 경사하강법 -gd
                - 확률적 경사하강법(일반적) - sgd - 복원추출(어제 청소한 사람도 오늘 청소할 수 있음)
                - 미니 배치 확률적 경사하강법 - mgd - 비복원추출(어제 청소한 사람은 오늘 청소에서 빼줌)
            5.학습률(커지면): 경사하강법의 속도(빨라짐) 정교함(떨어짐)을 결정

            6.직관적, 학습이 빠름

            7.변수설정x, 선형전제요구

         릿지(Ridge)/라쏘(Lasso): 선형회귀의 손실함수를 조작해서 나타남 / 알파(규제강도-과적합이 되지 않도록 통제하는 규제?)
            - ridge regression(규제)은 회귀 계수의 제곱합을 계산하는 방식
            - lasso는 회귀 계수의 절대값을 계산하는 방식

            - https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-feature-engineering-%EB%8B%A4%EC%A4%91-%ED%9A%8C%EA%B7%80 


            L1규제: 수렴이 불가능하기 때문에 특정항을 지워버릴때 사용 - 라소
            (목적을 위해서 베타를 죽일 생각이다)
            L2규제: 수렴이 가능하기는 해서 작게는 만드려고 하지만 아예 지우진 않음 - 릿지

로지스틱 회귀(Logit): 분류모델(기하모델이자 선형모델?)
              시그모이드 함수를 사용(x<0, 0<y<1/2 -> 0번 레이블| x=0, y=1/2 -> 점, 존재하지 않는 경우 | x>0, 1/2<y<1 -> 1번 레이블)
              장점: 회귀모델과 양립(호환) 가능, 확률로 표기 가능
              단점: 선형 전제 만족, 이진분류밖에 안됨
              하이퍼파라미터(HP): 규제의 정도(c, 결정?의 역할?도 추가?되는 규제?), 반복학습의 횟수(max=iter)

              선형 모델 방식을 기반으로 이진 분류를 수행하는 모델
              (단점: 이진분류밖에 못함 그렇지만 레이블이 2개만 있을때만 가능한 건 아님, 그 이상도 가능하긴 함->softmax/OVR/OVO이용->다진분류 가능해짐?)
              이름은 회귀지만 숫자 0과 1로 구분하는 분류 모델

              <선형모델방식을 분류에 사용하는 이유>
               -선형 모델은 간단한 함수식을 사용하기 때문에 학습 및 예측 속도가 빠르다
               -매우 큰 데이터 세트에서도 잘 동작한다
               -일반적으로 특성이 많을수록 더 잘 동작한다
                (그러나 특성이 적은 데이터에서는 다른 모델이 더 좋은 경우가 많다.)

OVO(OneVsOne) - 장점: softmax보다 성능이 좋음 
                단점: 시간이 오래걸림(n(n-1)/2), 결과값이 자기자신과 겹칠 수 있음(A - A, B: o) 직접 코딩을 해서 짜야함
레이블(n): A B C
model1: A vs B
model2: A vs C
model3: B vs C

OVR(OneVsRest, 하나와 그외) - 장점: OVO보다 계산을 덜해도 됨 -> OVO보다 빠름(n-1), 결과값이 자기자신과 겹치지 않음?(A - A: x)
                             단점: 데이터 불균형이 발생함
레이블(n): A B C 
model1: A vs A의 여집합
model2: B vs B의 여집합

손실함수: 어떤 모델이 최적화 되었는지 안되었는지 평가해주는 모델

기하모델: 좌표평면에 찍을 수 있다.
    +
확률모델: 빈도를 알 수 있다.-단위에 대한 정규화가 필요하지 않음
=기하적 확률(표본집단의 면적/모집단의 면적)